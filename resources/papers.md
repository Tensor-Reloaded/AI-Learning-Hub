# Recommended papers

A curated selection of foundational, influential, and emerging papers in machine learning and deep learning.

---

## Table of Contents

- [Classical Papers](#classical-papers)
- [Optimization](#optimization)
- [Architectures](#architectures)
- [Data Augmentation](#data-augmentation)
- [Reinforcement Learning (RL)](#reinforcement-learning-rl)
- [Self-Supervised Learning (SSL)](#self-supervised-learning-ssl)
- [Computer Vision (CV)](#computer-vision-cv)
- [Natural Language Processing (NLP)](#natural-language-processing-nlp)

---

## Classical Papers

Classical papers worth reading and understanding deeply.

- [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) (AlexNet, 2012)
- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385) (ResNet, 2015)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) (Transformer, 2017)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/pdf?id=YicbFdNTTy) (ViT, 2020)


---

## Optimization

* [Adam: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980) (Adam, 2015)
* [Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/pdf/1711.00489) (2017)
* [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/pdf/2010.01412) (SAMOptim, 2020)
* [94% on CIFAR-10 in 3.29 Seconds on a Single GPU](https://arxiv.org/pdf/2404.00498) (2024)
* [Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732) (2022)

Papers proposing efficient or alternative attention mechanisms.
- [Generating Long Sequences with Sparse Transformers](https://arxiv.org/pdf/1904.10509) (SparseAttention, 2019)
- [Performer: Rethinking Attention with Fast Linear Attention](https://arxiv.org/pdf/2009.14794) (Performer, 2020)
- [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/2006.04768) (Linformer, 2020)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135) (FlashAttention, 2022)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/pdf/2307.08691) (FlashAttention-2, 2023)
- [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://arxiv.org/pdf/2407.08608) (FlashAttention-3, 2024)
- [Radial Attention: O(n\log n) Sparse Attention with Energy Decay for Long Video Generation](https://arxiv.org/pdf/2506.19852) (RadialAttention, 2025)

---

## Architectures

- [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/pdf/2405.04517) (xLSTM, 2024)
- [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/pdf/2111.00396) (S4, 2022)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752) (Mamba, 2023)
- [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/pdf/2405.21060) (Mamba-2, 2024)
- [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/pdf/2302.10866) (Hyena, 2023)
- [Perceiver: General Perception with Iterative Attention](https://arxiv.org/pdf/2103.03206) (Perceiver, 2021)
- [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/pdf/2107.14795) (PerceiverIO, 2021)

---

## Data Augmentation

Key papers proposing impactful data augmentation strategies.

- [RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/pdf/1909.13719) (RandAug, 2019)
- [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/pdf/1710.09412) (MixUp, 2017)
- [CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features](https://arxiv.org/pdf/1905.04899) (CutMix, 2019)

---

## Reinforcement Learning (RL)


- [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602) (Atari, 2013)
- TODO: ...
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948) (DeepSeek-R1, 2025)

---

## Self-Supervised Learning (SSL)


- [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709) (SimCLR, 2020)
- [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/pdf/2006.07733) (BYOL, 2020)
- [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/pdf/2104.14294) (DINO, 2021)
- [**Self-supervised learning: The dark matter of intelligence**](https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/) (LeCun's Dark Matter, 2021)
- [A Path Towards Autonomous Machine Intelligence](https://openreview.net/pdf?id=BZ5a1r-kVsf) (JEPA, 2022)

---

## Computer Vision (CV)

Landmark papers in object detection, classification, and modern CV techniques.

- [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/pdf/1311.2524) (R-CNN, 2013)
- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497) (Faster R-CNN, 2015)
- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597) (U-Net, 2015)
- [Mask R-CNN](https://arxiv.org/pdf/1703.06870) (Mask R-CNN, 2017)
- [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640) (YOLO, 2015)
- [YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/pdf/2004.10934) (YOLOv4, 2020)
- [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872) (DETR, 2020)
- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030) (Swin, 2021)
- [Segment Anything](https://arxiv.org/pdf/2304.02643) (SAM, 2023)
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020) (CLIP, 2021)

---

## Natural Language Processing (NLP)

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805) (BERT, 2018)
- TODO
