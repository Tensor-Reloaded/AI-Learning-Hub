{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch/ComplexYetSimpleTrainingPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNL7l79tOg8N"
      },
      "source": [
        "# A complex yet simple efficient training pipeline for CIFAR-10\n",
        "\n",
        "This pipeline serves an educational purpose, hence that's why it's complex yet simple.\n",
        "\n",
        "For a more complex and more efficient training pipeline for CIFAR-10, do check [CIFAR-10 speedruns: 94% in 2.6 seconds and 96% in 27 seconds](https://github.com/KellerJordan/cifar10-airbench)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3q_jy9POg8P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import timm\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dovlmJqOg8Q"
      },
      "source": [
        "First we define some configuration variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVDFRFM0Og8R"
      },
      "outputs": [],
      "source": [
        "disable_compile = False\n",
        "compile_is_slower = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6uR-23mOg8S"
      },
      "source": [
        "### Efficient in-memory dataset wrapper for caching\n",
        "\n",
        "Beware that this dataset keeps all data in memory. If it is too large, we might opt to cache the data on the disk and read it in `__getitem__()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZngZVeBsOg8S"
      },
      "outputs": [],
      "source": [
        "class CachedDataset(Dataset):\n",
        "    def __init__(self, dataset: Dataset, runtime_transforms: v2.Transform | None):\n",
        "        # This operation caches all transformations from the wrapped dataset. Stores the results as a Tuple\n",
        "        # instead of list, decreasing memory usage. Tuples also have faster indexing, even though it is negligible.\n",
        "        self.dataset = tuple([x for x in dataset])\n",
        "\n",
        "        # These are the runtime transformations that can't be cached. Usually, they involve randomness which is a\n",
        "        # form of regularization for the network, and caching the randomness usually results in overfitting.\n",
        "        self.runtime_transforms = runtime_transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image, label = self.dataset[i]\n",
        "        if self.runtime_transforms is None:\n",
        "            return image, label\n",
        "        # We clone the data here, otherwise the runtime transforms might corrupt our data. They really do!\n",
        "        # You should never trust your users, even if they are yourself.\n",
        "        return self.runtime_transforms(image.clone()), label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okXAgQ-DOg8T"
      },
      "source": [
        "This is the classification model, which leverages PyTorch Image Models to create backbones.\n",
        "\n",
        "Beware that not all backbones have a fully connected (fc) layer at the end. Some of them do, especially the resnet variants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMzq8kqjOg8U"
      },
      "outputs": [],
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, backbone_name: str = 'resnet18', num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.backbone_name = backbone_name\n",
        "        self.backbone = timm.create_model(backbone_name, pretrained=False)\n",
        "        self.backbone.fc = nn.Linear(self.backbone.fc.weight.size(1), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOhQ2VIMOg8V"
      },
      "source": [
        "The comments are self-explainatory. If you do not know what a transformation does, the official documentation is your friend.\n",
        "Reading documentation helps your brain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx8_Z7dPOg8W"
      },
      "outputs": [],
      "source": [
        "def get_dataset(data_path: str, is_train: bool):\n",
        "    # These transformations are cached.\n",
        "    initial_transforms = [\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        "    normalize = v2.Normalize(mean=(0.491, 0.482, 0.446), std=(0.247, 0.243, 0.261), inplace=True)\n",
        "    # We use the inplace flag because we can safely change the tensors inplace when normalize is used.\n",
        "    # For is_train=False, we can safely change the tensors inplace because we do it only once, when caching.\n",
        "    # For is_train=True, we can safely change the tensors inplace because we clone the cached tensors first.\n",
        "\n",
        "    if is_train:\n",
        "        # We could have used RandomCrop with padding. But we are smart, and we know we cache the initial_transforms so\n",
        "        # we don't compute them during runtime. Therefore, we do the padding beforehand, and apply cropping only at\n",
        "        # runtime\n",
        "        initial_transforms.append(\n",
        "            v2.Pad(padding=4, fill=0.5)\n",
        "            # Why do we fill with 0.5? That's a good question, you should experiment with the fill value.\n",
        "        )\n",
        "        runtime_transforms = v2.Compose([\n",
        "            # For curious people: check whether RandomCrops returns a copy of its input, or a view\n",
        "            v2.RandomCrop(size=32),\n",
        "            v2.RandomHorizontalFlip(),\n",
        "            v2.RandomErasing(scale=(0.01, 0.15), value=0.5, inplace=True),\n",
        "            # If we use inplace here, it might modify the cached image. That's why we clone it.\n",
        "            # Why do we fill with 0.5? See above.\n",
        "            normalize,\n",
        "        ])\n",
        "    else:\n",
        "        initial_transforms.append(normalize)\n",
        "        runtime_transforms = None\n",
        "\n",
        "    # Q: How to make this faster?\n",
        "    # A: Use batched runtime transformations.\n",
        "\n",
        "    cifar10 = CIFAR10(root=data_path, train=is_train, transform=v2.Compose(initial_transforms), download=True)\n",
        "    return CachedDataset(cifar10, runtime_transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6vMzfV2Og8W"
      },
      "outputs": [],
      "source": [
        "def get_cutmix_or_mixup(num_classes: int = 10):\n",
        "    return v2.RandomChoice([\n",
        "        v2.CutMix(num_classes=num_classes),  # See the CutMix paper\n",
        "        v2.MixUp(num_classes=num_classes),  # See the MixUp paper\n",
        "        v2.Identity(),  # A third of all times, don't use neither CutMix nor MixUp.\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV1rocahOg8X"
      },
      "source": [
        "Below we have the trainer class. Carefully read the code and the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCROS3koOg8X"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model: ClassificationModel, optimizer: Optimizer, criterion: nn.Module, batch_size: int = 32,\n",
        "                 val_batch_size: int = 500, disable_tqdm: bool = False, save_path: str = \"best.pth\"):\n",
        "        self.device = torch.accelerator.current_accelerator()\n",
        "        print(f\"Using device {self.device}\")\n",
        "\n",
        "        # Efficiency stuff\n",
        "        if self.device.type == \"cuda\":\n",
        "            # This flag tells pytorch to use the cudnn auto-tuner to find the most efficient convolution algorithm for\n",
        "            # This training.\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            # Check this: https://docs.pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
        "            torch.set_float32_matmul_precision('high')\n",
        "\n",
        "        self.model = model.to(self.device)\n",
        "        if disable_compile or compile_is_slower:\n",
        "            # torch.jit.script is still a very good option, often faster than torch.compile, especially on windows\n",
        "            self.model = torch.jit.script(model)\n",
        "        else:\n",
        "            # This compiles the model. See https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html\n",
        "            self.model.compile()\n",
        "            # This compiles the step function\n",
        "            self.step = torch.compile(self.step)\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion.to(self.device)\n",
        "        self.scheduler = StepLR(self.optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "        train_dataset = get_dataset(\"./data\", True)\n",
        "        val_dataset = get_dataset(\"./data\", False)\n",
        "\n",
        "        # We need to shuffle the data during training. We also need to drop last, otherwise it will hurt the performance\n",
        "        # of torch.compile.\n",
        "        # Q: What if I am not using torch.compile? Can I set drop_last to False?\n",
        "        # A: Think of the last batch, with fewer elements than the batch size you selected during training.\n",
        "        #    If you were to calculate the gradient for it, how would the gradient differ from the gradient of a batch\n",
        "        #    with all elements?\n",
        "        # The answer is left as an exercise to the reader.\n",
        "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        self.val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
        "\n",
        "        self.disable_tqdm = disable_tqdm\n",
        "        self.save_path = save_path\n",
        "        self.best_va_acc = 0.0\n",
        "\n",
        "        self.cutmix_mixup = get_cutmix_or_mixup(self.model.backbone.fc.weight.shape[0])\n",
        "\n",
        "    def step(self, data: torch.Tensor, target: torch.Tensor):\n",
        "        predicted = self.model(data)\n",
        "        loss = self.criterion(predicted, target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        return predicted, loss\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for data, target in tqdm(self.train_loader, desc=\"Training\", leave=False, disable=self.disable_tqdm):\n",
        "            # We apply cutmix or mixup. We need to pass both the data and the labels, because this kind of DA changes\n",
        "            # The targets from hard labels to soft labels. Check the DA notebook for more details.\n",
        "            data, target = self.cutmix_mixup(data, target)\n",
        "\n",
        "            # Using non_blocking=True means that the transfer from cpu RAM to device RAM is done asynchronously.\n",
        "            # Works when using pin_memory=True. For more details, check the references for pinning memory.\n",
        "            predicted, loss = self.step(\n",
        "                data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True))\n",
        "\n",
        "            if target.ndim > 1:\n",
        "                # We do this when cutmix or mixup was used, transforming the hard labels into soft labels\n",
        "                target = target.argmax(1)\n",
        "            # This metric is actually an approximation of an accuracy, we are checking whether the dominant class\n",
        "            # predicted by the model is also equal to the dominant soft label\n",
        "            # The reason we are moving the data from device back to CPU is because these calculations are usually\n",
        "            # faster on CPU for small batch sizes\n",
        "            # We use detach because we tell the autograd engine to not track the gradients for predicted anymore\n",
        "            correct += predicted.detach().cpu().argmax(1).eq(target).sum().item()\n",
        "            total += data.size(0)\n",
        "            total_loss += float(loss.item())\n",
        "\n",
        "        return correct / total, total_loss / len(self.train_loader)\n",
        "\n",
        "    # Here we use the inference_mode. We are telling pytorch we are doing just inference, we don't need to track\n",
        "    # tensor operations with the Autograd engine for automatic differentiation. This is also what torch.no_grad() does.\n",
        "    # torch.inference_mode() = torch.no_grad() + promising torch we will never use any tensor created in this scope in\n",
        "    # autograd tracked operations.\n",
        "    # This promise allows additional optimizations, such as removing version tracking from tensors. If we violate the\n",
        "    # promise, and use a tensor created in the inference_mode scope in an operation for which we need to calculate the\n",
        "    # gradient, we should expect errors.\n",
        "    # Recapitulating:\n",
        "    #  * If we will never use Autograd, inference_mode is more optimized.\n",
        "    #  * If we use Autograd, but just don't want to track some operations using Autograd, use no_grad.\n",
        "    @torch.inference_mode()\n",
        "    def val(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for data, target in tqdm(self.val_loader, desc=\"Validation\", leave=False, disable=self.disable_tqdm):\n",
        "            # We don't need to move the targets to device for validation.\n",
        "            predicted = self.model(data.to(self.device, non_blocking=True))\n",
        "            total_loss += float(self.criterion(predicted, target.to(self.device, non_blocking=True)).item())\n",
        "\n",
        "            # Here we don't need to argmax the target, because we have hard labels. We don't use DA during validation.\n",
        "            # We don't need to detach, because we are already in inference_mode\n",
        "            correct += predicted.cpu().argmax(1).eq(target).sum().item()\n",
        "            total += data.size(0)\n",
        "\n",
        "        return correct / total, total_loss / len(self.val_loader)\n",
        "\n",
        "    def run(self, epochs: int):\n",
        "        with tqdm(range(epochs), desc=\"Training\") as pbar:\n",
        "            for _ in pbar:\n",
        "                tr_acc, tr_loss = self.train()\n",
        "                va_acc, va_loss = self.val()\n",
        "                self.scheduler.step()\n",
        "                if va_acc > self.best_va_acc:\n",
        "                    self.best_va_acc = va_acc\n",
        "                    torch.save({\n",
        "                        'model_state_dict': self.model.state_dict(),\n",
        "                        'model_name': self.model.backbone_name,\n",
        "                        'num_classes': int(self.model.backbone.fc.weight.shape[0]),\n",
        "                    }, self.save_path)\n",
        "\n",
        "                pbar.set_postfix(train_acc=tr_acc, train_loss=round(tr_loss, 3), val_acc=va_acc,\n",
        "                                 val_loss=round(va_loss, 3), best_val_acc=self.best_va_acc)\n",
        "        # We use tqdm to have a progress bar for the epochs. We disable inner progress bars on jupyter notebooks,\n",
        "        # because either they produce a lot of output, or disable loading the notebook on GitHub.\n",
        "        # If you run this script on a terminal, you can enable the inner progress bars.\n",
        "        # Some more details about efficiency:\n",
        "        #  * Using pin_memory=True in the DataLoader usually increases the data transfer speed from\n",
        "        #    CPU RAM to GPU RAM, using pinned memory. More details in the official documentation.\n",
        "        #    The downside is that pinned memory is a limited resource, and allocating too much of it can lead to\n",
        "        #    system instability. Therefore, monitor your system when using pin_memory=True.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mxX49B9Og8Y"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    model = ClassificationModel()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, nesterov=True, weight_decay=0.01, fused=True)\n",
        "    # optimizer = torch.optim.AdamW(model.parameters(), fused=True)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    Trainer(model, optimizer, criterion, save_path=\"best.pth\", disable_tqdm=True).run(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTLTAn5pOg8Y"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # If torch.compile is actually slower on your machine.\n",
        "    # On my machine, 10 epochs with torch.compile take 5 minutes. With torch.jit.script, they take 4 minutes.\n",
        "    # Based on my experience, for small models without custom kernels, torch.jit.script is usually faster.\n",
        "    compile_is_slower = True\n",
        "\n",
        "    if os.name == \"nt\":\n",
        "        print(\"torch.compile is disabled\")\n",
        "        disable_compile = True\n",
        "    else:\n",
        "        print(\"torch.compile is enabled\" + (\" BUT not really used\" if compile_is_slower else \"\"))\n",
        "        if torch.accelerator.is_available():\n",
        "            torch._dynamo.config.capture_scalar_outputs = True\n",
        "\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q47ymPCBOg8Z"
      },
      "source": [
        "The full training script is available in [complex_yet_simple_training_pipeline.py](./complex_yet_simple_training_pipeline.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r36R_vT5Og8Z"
      },
      "source": [
        "## Excercises\n",
        "\n",
        "1. Create your own efficient training pipeline for CIFAR-10.\n",
        "2. Adapt your pipeline (and this pipeline) to use some batched transformations. Measure the speedup!\n",
        "3. Adapt your pipeline (and this pipeline) to include Automatic Mixed Precision. Read the documentation first!\n",
        "4. Adjust your pipeline (or this pipeline) to achieve 96% on CIFAR-10 (hard). You may change the model, but pretrained weights are forbidden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F31jwuDOg8Z"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVwcPWNsOg8a"
      },
      "source": [
        "| All     | [advanced_pytorch/](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch) |\n",
        "|---------|-- |\n",
        "| Current | [A complex yet simple efficient training pipeline for CIFAR-10](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch/ComplexYetSimpleTrainingPipeline.ipynb) |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}