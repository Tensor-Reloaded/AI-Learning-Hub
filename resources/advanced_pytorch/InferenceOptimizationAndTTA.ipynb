{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch/InferenceOptimizationAndTTA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Egajx6dH5y"
      },
      "source": [
        "# Inference Optimization and Test Time Augmentation (TTA)\n",
        "\n",
        "This is the inference pipeline for the model trained in  [A complex yet simple efficient training pipeline for CIFAR-10](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch/ComplexYetSimpleTrainingPipeline.ipynb).\n",
        "\n",
        "In this Notebook you will learn about simple inference optimization you can do while serving pytorch models.\n",
        "\n",
        "For more advanced information, please check [TorchScript](https://docs.pytorch.org/docs/stable/jit.html) and [onnx](https://onnxruntime.ai/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timed-decorator"
      ],
      "metadata": {
        "id": "AI71oFHGfMUD",
        "outputId": "69f5fbc1-502d-484d-f8f2-747f1d6f0383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timed-decorator\n",
            "  Downloading timed_decorator-1.6.1-py3-none-any.whl.metadata (18 kB)\n",
            "Downloading timed_decorator-1.6.1-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: timed-decorator\n",
            "Successfully installed timed-decorator-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from itertools import product\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "from prettytable import PrettyTable\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.transforms.v2.functional import hflip\n",
        "from timed_decorator.simple_timed import timed\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6cL13rU8dK4k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same model trained in the previous training notebook"
      ],
      "metadata": {
        "id": "JIicpzj-eJQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, backbone_name: str = \"resnet18\", num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(backbone_name, pretrained=False)\n",
        "        self.backbone.fc = nn.Linear(self.backbone.fc.weight.size(1), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n"
      ],
      "metadata": {
        "id": "dPF50f3WdQHi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method creates a model on a device and prepares it for serving.\n",
        "Depending on the optimization type, the model is optimized using different TorchScript jit utilities, or even compiled using torch.compile.\n",
        "\n",
        "For a detailed description of each jit/compile method, please check the official documentation and official tutorials."
      ],
      "metadata": {
        "id": "4WPBgKrlePfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_path: str, device: torch.device, model_type: str):\n",
        "    model_data = torch.load(model_path, map_location=device, weights_only=True)\n",
        "\n",
        "    model = ClassificationModel(model_data[\"model_name\"], model_data[\"num_classes\"])\n",
        "    model = model.to(device)\n",
        "    model.load_state_dict(model_data[\"model_state_dict\"])\n",
        "    model.eval()\n",
        "\n",
        "    if model_type == \"raw model\":\n",
        "        return model\n",
        "    if model_type == \"scripted model\":\n",
        "        return torch.jit.script(model)\n",
        "    if model_type == \"traced model\":\n",
        "        return torch.jit.trace(model, torch.rand((5, 3, 32, 32), device=device))\n",
        "    if model_type == \"frozen model\":\n",
        "        return torch.jit.freeze(torch.jit.script(model))\n",
        "    if model_type == \"optimized for inference\":\n",
        "        return torch.jit.optimize_for_inference(torch.jit.script(model))\n",
        "    if model_type == \"compiled model\":\n",
        "        if os.name == \"nt\":\n",
        "            print(\"torch.compile is not supported on Windows. Try Linux or WSL instead.\")\n",
        "            return model\n",
        "        return torch.compile(model)\n",
        "    raise RuntimeError(\"std::unreachable\")\n"
      ],
      "metadata": {
        "id": "9XM8OP98dTdc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function performs inference and TTA, while measuring the elapsed time.\n",
        "\n",
        "There are 4 versions of TTA:\n",
        "* no TTA\n",
        "* mirroring: performs a horizontal flip for the input images, doing an additional inference pass\n",
        "* translate: performs 8 translations of the input images, doing 8 additional inference passes\n",
        "* mirroring_and_translate"
      ],
      "metadata": {
        "id": "fRQxsuyafJbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed(stdout=False, return_time=True, use_seconds=True)\n",
        "def tta_inference(model, batches: Tuple[Tuple[torch.Tensor, torch.Tensor], ...], device: torch.device,\n",
        "                  tta_type: str) -> float:\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for data, target in batches:\n",
        "        data = data.to(device)\n",
        "\n",
        "        predicted = model(data)\n",
        "        if tta_type == \"mirroring\":\n",
        "            predicted += model(hflip(data))\n",
        "        elif tta_type == \"translate\":\n",
        "            padding_size = 2\n",
        "            image_size = 32\n",
        "            # We pad using the same value the model has seen during training\n",
        "            padded = v2.functional.pad(data, [padding_size], fill=0.5)\n",
        "            for i in [-2, 0, 2]:\n",
        "                for j in [-2, 0, 2]:\n",
        "                    if i == 0 and j == 0:\n",
        "                        continue\n",
        "                    x = padding_size + i\n",
        "                    y = padding_size + j\n",
        "                    predicted += model(padded[:, :, x:x + image_size, y:y + image_size])\n",
        "        elif tta_type == \"mirroring_and_translate\":\n",
        "            padding_size = 2\n",
        "            image_size = 32\n",
        "            padded = v2.functional.pad(data, [padding_size], fill=0.5)\n",
        "            for i in [-2, 0, 2]:\n",
        "                for j in [-2, 0, 2]:\n",
        "                    if i == 0 and j == 0:\n",
        "                        continue\n",
        "                    x = padding_size + i\n",
        "                    y = padding_size + j\n",
        "                    aux = padded[:, :, x:x + image_size, y:y + image_size]\n",
        "                    predicted += model(aux)\n",
        "                    predicted += model(hflip(aux))\n",
        "\n",
        "        correct += (predicted.cpu().argmax(dim=1) == target).sum().item()\n",
        "        total += data.size(0)\n",
        "\n",
        "    return round(correct / total, 4)\n"
      ],
      "metadata": {
        "id": "lLjSIb5bfJ_0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the automated mixed precision module to automatically cast to our desired data type. We measure the accuracy and the elapsed time of the configuration."
      ],
      "metadata": {
        "id": "0yn2xm30hFJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, batches: Tuple[Tuple[torch.Tensor, torch.Tensor], ...], device: torch.device, tta_type: str,\n",
        "              dtype: torch.dtype, model_type: str) -> Tuple[float, float]:\n",
        "    enable_autocast = device.type == \"cuda\" and dtype != torch.float32\n",
        "    # Autocast is slow for cpu, so we disable it.\n",
        "    # Also, if the device type is mps, autocast might not work (?)\n",
        "    accuracy, elapsed = \"N/A\", \"N/A\"\n",
        "    try:\n",
        "\n",
        "        with torch.autocast(device_type=device.type, dtype=dtype, enabled=enable_autocast), torch.inference_mode():\n",
        "            accuracy, elapsed = tta_inference(model, batches, device, tta_type)\n",
        "    except:\n",
        "        # Debug only\n",
        "\n",
        "        # import traceback\n",
        "        # traceback.print_exc()\n",
        "        print(f\"Model type {model_type} failed on {dtype} on {device.type}\")\n",
        "\n",
        "    return accuracy, elapsed\n"
      ],
      "metadata": {
        "id": "jtEujsCHhBpY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepare the data and pack it into a tuple for fast dataloading and pre-batching. During inference, data is read directly from memory."
      ],
      "metadata": {
        "id": "cf48GMYehYAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data_path: str) -> Tuple[Tuple[torch.Tensor, torch.Tensor], ...]:\n",
        "    transforms = v2.Compose([\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(mean=(0.491, 0.482, 0.446), std=(0.247, 0.243, 0.261), inplace=True)\n",
        "    ])\n",
        "    dataset = CIFAR10(root=data_path, train=False, transform=transforms, download=True)\n",
        "    dataloader = DataLoader(dataset, batch_size=200)\n",
        "    return tuple([x for x in dataloader])\n"
      ],
      "metadata": {
        "id": "9jtEACrdhV1_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the speed test, and the results from my computer. The results for Google Colab are below."
      ],
      "metadata": {
        "id": "yBiEF9LZhpzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_speed_test(data: Tuple[Tuple[torch.Tensor, torch.Tensor], ...],\n",
        "                  model_types: Tuple[str, ...],\n",
        "                  dtypes: Tuple[torch.dtype, ...],\n",
        "                  tta_types: Tuple[str, ...],\n",
        "                  devices: Tuple[torch.device | None, ...],\n",
        "                  model_path: str):\n",
        "    tta_type = \"none\"\n",
        "    with tqdm(total=len(devices) * len(dtypes) * len(model_types), desc=\"Speed experiments\") as tbar:\n",
        "        for device, dtype in product(devices, dtypes):\n",
        "            if device is None:\n",
        "                tbar.update(len(model_types))\n",
        "                continue\n",
        "            speed_results = PrettyTable()\n",
        "            speed_results.field_names = [\"Device\", \"Dtype\", \"TTA Type\", \"Model Type\", \"Accuracy\", \"Elapsed\"]\n",
        "\n",
        "            for model_type in model_types:\n",
        "                model = create_model(model_path, device, model_type)\n",
        "                accuracy, elapsed = inference(model, data, device, tta_type, dtype, model_type)\n",
        "                speed_results.add_row([device, dtype, tta_type, model_type, accuracy, elapsed])\n",
        "                tbar.update()\n",
        "\n",
        "            print(speed_results)\n",
        "\n",
        "    # CUDA Results\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # | Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # |  cuda  | torch.bfloat16 |   none   |        raw model        |  0.8627  |  0.74056251 |\n",
        "    # |  cuda  | torch.bfloat16 |   none   |      scripted model     |  0.8627  | 0.550711881 |\n",
        "    # |  cuda  | torch.bfloat16 |   none   |      scripted model     |  0.8627  | 0.466999062 |\n",
        "    # |  cuda  | torch.bfloat16 |   none   |       traced model      |  0.8627  | 0.505114635 |\n",
        "    # |  cuda  | torch.bfloat16 |   none   |       traced model      |  0.8627  | 0.497691016 |\n",
        "    # |  cuda  | torch.bfloat16 |   none   |       frozen model      |  0.8618  | 0.630178739 |\n",
        "    # |  cuda  | torch.bfloat16 |   none   |       frozen model      |  0.8618  | 0.431321397 |\n",
        "    # |  cuda  | torch.bfloat16 |   none   | optimized for inference |   N/A    |     N/A     |\n",
        "    # |  cuda  | torch.bfloat16 |   none   | optimized for inference |   N/A    |     N/A     |\n",
        "    # |  cuda  | torch.bfloat16 |   none   |      compiled model     |  0.863   |  1.37197609 |\n",
        "    # |  cuda  | torch.bfloat16 |   none   |      compiled model     |  0.863   | 0.439737346 |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # | Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # |  cuda  | torch.float16  |   none   |        raw model        |  0.8629  | 0.934939784 |\n",
        "    # |  cuda  | torch.float16  |   none   |      scripted model     |  0.8629  | 0.776701284 |\n",
        "    # |  cuda  | torch.float16  |   none   |      scripted model     |  0.8629  |  0.65642132 |\n",
        "    # |  cuda  | torch.float16  |   none   |       traced model      |  0.8629  | 0.770792187 |\n",
        "    # |  cuda  | torch.float16  |   none   |       traced model      |  0.8629  | 0.761494488 |\n",
        "    # |  cuda  | torch.float16  |   none   |       frozen model      |  0.8629  | 0.449910122 |\n",
        "    # |  cuda  | torch.float16  |   none   |       frozen model      |  0.8629  | 0.428042867 |\n",
        "    # |  cuda  | torch.float16  |   none   | optimized for inference |   N/A    |     N/A     |\n",
        "    # |  cuda  | torch.float16  |   none   | optimized for inference |   N/A    |     N/A     |\n",
        "    # |  cuda  | torch.float16  |   none   |      compiled model     |  0.863   | 1.041609176 |\n",
        "    # |  cuda  | torch.float16  |   none   |      compiled model     |  0.863   | 0.304629578 |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # | Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # |  cuda  | torch.float32  |   none   |        raw model        |  0.8628  | 0.928239116 |\n",
        "    # |  cuda  | torch.float32  |   none   |      scripted model     |  0.8628  | 0.869112261 |\n",
        "    # |  cuda  | torch.float32  |   none   |      scripted model     |  0.8628  | 0.818328065 |\n",
        "    # |  cuda  | torch.float32  |   none   |       traced model      |  0.8628  | 0.831756814 |\n",
        "    # |  cuda  | torch.float32  |   none   |       traced model      |  0.8628  | 0.835166337 |\n",
        "    # |  cuda  | torch.float32  |   none   |       frozen model      |  0.8628  | 0.635774185 |\n",
        "    # |  cuda  | torch.float32  |   none   |       frozen model      |  0.8628  | 0.884842387 |\n",
        "    # |  cuda  | torch.float32  |   none   | optimized for inference |  0.8628  | 6.401095805 |\n",
        "    # |  cuda  | torch.float32  |   none   | optimized for inference |  0.8628  | 6.383807842 |\n",
        "    # |  cuda  | torch.float32  |   none   |      compiled model     |  0.8628  | 0.979224372 |\n",
        "    # |  cuda  | torch.float32  |   none   |      compiled model     |  0.8628  | 0.510377062 |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "\n",
        "    # CPU Results\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # | Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # |  cpu   | torch.bfloat16 |   none   |        raw model        |  0.8628  | 2.859445197 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   |      scripted model     |  0.8628  | 2.635952067 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   |      scripted model     |  0.8628  | 2.604736663 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   |       traced model      |  0.8628  | 2.631448843 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   |       traced model      |  0.8628  | 2.576900248 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   |       frozen model      |  0.8628  | 2.546161701 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   |       frozen model      |  0.8628  | 2.502300936 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   | optimized for inference |  0.8628  | 2.281604414 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   | optimized for inference |  0.8628  | 2.225087941 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   |      compiled model     |  0.8628  |  3.58207681 |\n",
        "    # |  cpu   | torch.bfloat16 |   none   |      compiled model     |  0.8628  | 1.722796112 |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # | Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # |  cpu   | torch.float16  |   none   |        raw model        |  0.8628  | 2.737279273 |\n",
        "    # |  cpu   | torch.float16  |   none   |      scripted model     |  0.8628  | 2.562341959 |\n",
        "    # |  cpu   | torch.float16  |   none   |      scripted model     |  0.8628  | 2.652842815 |\n",
        "    # |  cpu   | torch.float16  |   none   |       traced model      |  0.8628  | 2.639518142 |\n",
        "    # |  cpu   | torch.float16  |   none   |       traced model      |  0.8628  | 2.735255652 |\n",
        "    # |  cpu   | torch.float16  |   none   |       frozen model      |  0.8628  | 2.903561699 |\n",
        "    # |  cpu   | torch.float16  |   none   |       frozen model      |  0.8628  | 2.962546338 |\n",
        "    # |  cpu   | torch.float16  |   none   | optimized for inference |  0.8628  | 2.344554807 |\n",
        "    # |  cpu   | torch.float16  |   none   | optimized for inference |  0.8628  | 2.360003218 |\n",
        "    # |  cpu   | torch.float16  |   none   |      compiled model     |  0.8628  | 1.730791658 |\n",
        "    # |  cpu   | torch.float16  |   none   |      compiled model     |  0.8628  | 1.754020479 |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # | Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n",
        "    # |  cpu   | torch.float32  |   none   |        raw model        |  0.8628  |  2.65187362 |\n",
        "    # |  cpu   | torch.float32  |   none   |      scripted model     |  0.8628  | 2.620745015 |\n",
        "    # |  cpu   | torch.float32  |   none   |      scripted model     |  0.8628  | 2.583938025 |\n",
        "    # |  cpu   | torch.float32  |   none   |       traced model      |  0.8628  |  2.68518527 |\n",
        "    # |  cpu   | torch.float32  |   none   |       traced model      |  0.8628  | 2.670001929 |\n",
        "    # |  cpu   | torch.float32  |   none   |       frozen model      |  0.8628  | 2.853723278 |\n",
        "    # |  cpu   | torch.float32  |   none   |       frozen model      |  0.8628  | 2.903551512 |\n",
        "    # |  cpu   | torch.float32  |   none   | optimized for inference |  0.8628  |  2.47234354 |\n",
        "    # |  cpu   | torch.float32  |   none   | optimized for inference |  0.8628  | 2.308440241 |\n",
        "    # |  cpu   | torch.float32  |   none   |      compiled model     |  0.8628  |  1.78701703 |\n",
        "    # |  cpu   | torch.float32  |   none   |      compiled model     |  0.8628  | 1.771141438 |\n",
        "    # +--------+----------------+----------+-------------------------+----------+-------------+\n"
      ],
      "metadata": {
        "id": "AEzh63Q8homW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results from Google Colab CUDA"
      ],
      "metadata": {
        "id": "qG6P0eexj6W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# +--------+---------------+----------+-------------------------+----------+-------------+\n",
        "# | Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "# +--------+---------------+----------+-------------------------+----------+-------------+\n",
        "# |  cuda  | torch.float16 |   none   |        raw model        |  0.8628  | 0.456860901 |\n",
        "# |  cuda  | torch.float16 |   none   |      scripted model     |  0.8627  | 0.245520422 |\n",
        "# |  cuda  | torch.float16 |   none   |      scripted model     |  0.8627  |  0.24478505 |\n",
        "# |  cuda  | torch.float16 |   none   |       traced model      |  0.8628  | 0.297093739 |\n",
        "# |  cuda  | torch.float16 |   none   |       traced model      |  0.8628  | 0.298865424 |\n",
        "# |  cuda  | torch.float16 |   none   |       frozen model      |  0.8629  | 0.189003802 |\n",
        "# |  cuda  | torch.float16 |   none   |       frozen model      |  0.8629  | 0.188668645 |\n",
        "# |  cuda  | torch.float16 |   none   | optimized for inference |   N/A    |     N/A     |\n",
        "# |  cuda  | torch.float16 |   none   | optimized for inference |   N/A    |     N/A     |\n",
        "# |  cuda  | torch.float16 |   none   |      compiled model     |  0.8628  | 0.351826348 |\n",
        "# |  cuda  | torch.float16 |   none   |      compiled model     |  0.8628  | 0.245900978 |\n",
        "# +--------+---------------+----------+-------------------------+----------+-------------+\n",
        "\n",
        "# +--------+---------------+----------+-------------------------+----------+-------------+\n",
        "# | Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "# +--------+---------------+----------+-------------------------+----------+-------------+\n",
        "# |  cuda  | torch.float16 |   none   |        raw model        |  0.8628  | 0.456860901 |\n",
        "# |  cuda  | torch.float16 |   none   |      scripted model     |  0.8627  | 0.245520422 |\n",
        "# |  cuda  | torch.float16 |   none   |      scripted model     |  0.8627  |  0.24478505 |\n",
        "# |  cuda  | torch.float16 |   none   |       traced model      |  0.8628  | 0.297093739 |\n",
        "# |  cuda  | torch.float16 |   none   |       traced model      |  0.8628  | 0.298865424 |\n",
        "# |  cuda  | torch.float16 |   none   |       frozen model      |  0.8629  | 0.189003802 |\n",
        "# |  cuda  | torch.float16 |   none   |       frozen model      |  0.8629  | 0.188668645 |\n",
        "# |  cuda  | torch.float16 |   none   | optimized for inference |   N/A    |     N/A     |\n",
        "# |  cuda  | torch.float16 |   none   | optimized for inference |   N/A    |     N/A     |\n",
        "# |  cuda  | torch.float16 |   none   |      compiled model     |  0.8628  | 0.351826348 |\n",
        "# |  cuda  | torch.float16 |   none   |      compiled model     |  0.8628  | 0.245900978 |\n",
        "# +--------+---------------+----------+-------------------------+----------+-------------+\n",
        "\n",
        "# +--------+---------------+----------+-------------------------+----------+-------------+\n",
        "# | Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
        "# +--------+---------------+----------+-------------------------+----------+-------------+\n",
        "# |  cuda  | torch.float32 |   none   |        raw model        |  0.8628  | 0.612244879 |\n",
        "# |  cuda  | torch.float32 |   none   |      scripted model     |  0.8628  | 0.714759405 |\n",
        "# |  cuda  | torch.float32 |   none   |      scripted model     |  0.8628  | 0.589165576 |\n",
        "# |  cuda  | torch.float32 |   none   |       traced model      |  0.8628  | 0.616923644 |\n",
        "# |  cuda  | torch.float32 |   none   |       traced model      |  0.8628  | 0.614003439 |\n",
        "# |  cuda  | torch.float32 |   none   |       frozen model      |  0.8628  | 0.560715173 |\n",
        "# |  cuda  | torch.float32 |   none   |       frozen model      |  0.8628  | 0.559296638 |\n",
        "# |  cuda  | torch.float32 |   none   | optimized for inference |  0.8628  |  0.55052995 |\n",
        "# |  cuda  | torch.float32 |   none   | optimized for inference |  0.8628  | 0.551463229 |\n",
        "# |  cuda  | torch.float32 |   none   |      compiled model     |  0.8628  | 0.591989673 |\n",
        "# |  cuda  | torch.float32 |   none   |      compiled model     |  0.8628  | 0.589906088 |\n",
        "# +--------+---------------+----------+-------------------------+----------+-------------+"
      ],
      "metadata": {
        "id": "U2oZeFukjzPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results from Google Colab CPU"
      ],
      "metadata": {
        "id": "PA7mp_BekGUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# +--------+----------------+----------+-------------------------+----------+--------------+\n",
        "# | Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed    |\n",
        "# +--------+----------------+----------+-------------------------+----------+--------------+\n",
        "# |  cpu   | torch.bfloat16 |   none   |        raw model        |  0.8628  | 12.251049287 |\n",
        "# |  cpu   | torch.bfloat16 |   none   |      scripted model     |  0.8628  | 10.502742393 |\n",
        "# |  cpu   | torch.bfloat16 |   none   |      scripted model     |  0.8628  | 10.219771897 |\n",
        "# |  cpu   | torch.bfloat16 |   none   |       traced model      |  0.8628  | 9.444874722  |\n",
        "# |  cpu   | torch.bfloat16 |   none   |       traced model      |  0.8628  | 10.067524619 |\n",
        "# |  cpu   | torch.bfloat16 |   none   |       frozen model      |  0.8628  | 9.681582857  |\n",
        "# |  cpu   | torch.bfloat16 |   none   |       frozen model      |  0.8628  |  9.74893516  |\n",
        "# |  cpu   | torch.bfloat16 |   none   | optimized for inference |  0.8628  | 8.125695756  |\n",
        "# |  cpu   | torch.bfloat16 |   none   | optimized for inference |  0.8628  |  6.87960141  |\n",
        "# |  cpu   | torch.bfloat16 |   none   |      compiled model     |  0.8628  | 9.944255357  |\n",
        "# |  cpu   | torch.bfloat16 |   none   |      compiled model     |  0.8628  | 9.934951253  |\n",
        "# +--------+----------------+----------+-------------------------+----------+--------------+\n",
        "\n",
        "# +--------+---------------+----------+-------------------------+----------+--------------+\n",
        "# | Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed    |\n",
        "# +--------+---------------+----------+-------------------------+----------+--------------+\n",
        "# |  cpu   | torch.float16 |   none   |        raw model        |  0.8628  | 10.195400691 |\n",
        "# |  cpu   | torch.float16 |   none   |      scripted model     |  0.8628  | 10.132698804 |\n",
        "# |  cpu   | torch.float16 |   none   |      scripted model     |  0.8628  | 9.415232976  |\n",
        "# |  cpu   | torch.float16 |   none   |       traced model      |  0.8628  | 9.189693126  |\n",
        "# |  cpu   | torch.float16 |   none   |       traced model      |  0.8628  | 9.873023653  |\n",
        "# |  cpu   | torch.float16 |   none   |       frozen model      |  0.8628  | 9.554109585  |\n",
        "# |  cpu   | torch.float16 |   none   |       frozen model      |  0.8628  | 9.601258977  |\n",
        "# |  cpu   | torch.float16 |   none   | optimized for inference |  0.8628  | 7.365994933  |\n",
        "# |  cpu   | torch.float16 |   none   | optimized for inference |  0.8628  | 6.933587296  |\n",
        "# |  cpu   | torch.float16 |   none   |      compiled model     |  0.8628  | 10.214564148 |\n",
        "# |  cpu   | torch.float16 |   none   |      compiled model     |  0.8628  | 10.10502345  |\n",
        "# +--------+---------------+----------+-------------------------+----------+--------------+\n",
        "\n",
        "# +--------+---------------+----------+-------------------------+----------+--------------+\n",
        "# | Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed    |\n",
        "# +--------+---------------+----------+-------------------------+----------+--------------+\n",
        "# |  cpu   | torch.float32 |   none   |        raw model        |  0.8628  | 10.396136796 |\n",
        "# |  cpu   | torch.float32 |   none   |      scripted model     |  0.8628  | 10.187017377 |\n",
        "# |  cpu   | torch.float32 |   none   |      scripted model     |  0.8628  | 9.242592588  |\n",
        "# |  cpu   | torch.float32 |   none   |       traced model      |  0.8628  |  9.72705402  |\n",
        "# |  cpu   | torch.float32 |   none   |       traced model      |  0.8628  | 11.291153583 |\n",
        "# |  cpu   | torch.float32 |   none   |       frozen model      |  0.8628  | 10.222028228 |\n",
        "# |  cpu   | torch.float32 |   none   |       frozen model      |  0.8628  | 9.580895448  |\n",
        "# |  cpu   | torch.float32 |   none   | optimized for inference |  0.8628  |  7.60580902  |\n",
        "# |  cpu   | torch.float32 |   none   | optimized for inference |  0.8628  | 6.929277715  |\n",
        "# |  cpu   | torch.float32 |   none   |      compiled model     |  0.8628  | 9.964773122  |\n",
        "# |  cpu   | torch.float32 |   none   |      compiled model     |  0.8628  | 9.956867878  |\n",
        "# +--------+---------------+----------+-------------------------+----------+--------------+"
      ],
      "metadata": {
        "id": "Rf_VzCJgkI83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have the TTA test. TTA can really improve the results."
      ],
      "metadata": {
        "id": "OUkFkii5hySW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_tta_test(data: Tuple[Tuple[torch.Tensor, torch.Tensor], ...],\n",
        "                model_types: Tuple[str, ...],\n",
        "                dtypes: Tuple[torch.dtype, ...],\n",
        "                tta_types: Tuple[str, ...],\n",
        "                devices: Tuple[torch.device | None, ...],\n",
        "                model_path: str):\n",
        "    tta_results = PrettyTable()\n",
        "    tta_results.field_names = [\"Device\", \"Dtype\", \"TTA Type\", \"Model Type\", \"Accuracy\", \"Elapsed\"]\n",
        "\n",
        "    device = devices[0] if devices[0] is not None else devices[1]\n",
        "    model_type = \"scripted model\"\n",
        "\n",
        "    for dtype, tta_type in tqdm(tuple(product(dtypes, tta_types)), desc=\"TTA experiments\"):\n",
        "        if device is None:\n",
        "            continue\n",
        "        model = create_model(model_path, device, model_type)\n",
        "        accuracy, elapsed = inference(model, data, device, tta_type, dtype, model_type)\n",
        "        tta_results.add_row([device, dtype, tta_type, model_type, accuracy, elapsed])\n",
        "\n",
        "    print(tta_results)\n",
        "\n",
        "    # +--------+----------------+-------------------------+----------------+----------+-------------+\n",
        "    # | Device |     Dtype      |         TTA Type        |   Model Type   | Accuracy |   Elapsed   |\n",
        "    # +--------+----------------+-------------------------+----------------+----------+-------------+\n",
        "    # |  cuda  | torch.bfloat16 |           none          | scripted model |  0.8627  |  0.86833901 |\n",
        "    # |  cuda  | torch.bfloat16 |        mirroring        | scripted model |  0.8729  | 0.369391463 |\n",
        "    # |  cuda  | torch.bfloat16 |        translate        | scripted model |  0.8733  | 1.242899479 |\n",
        "    # |  cuda  | torch.bfloat16 | mirroring_and_translate | scripted model |  0.8783  | 2.240457862 |\n",
        "    # |  cuda  | torch.float16  |           none          | scripted model |  0.8629  | 0.359551112 |\n",
        "    # |  cuda  | torch.float16  |        mirroring        | scripted model |  0.8729  | 0.314461259 |\n",
        "    # |  cuda  | torch.float16  |        translate        | scripted model |  0.8733  | 1.377882807 |\n",
        "    # |  cuda  | torch.float16  | mirroring_and_translate | scripted model |  0.8783  |  2.50420385 |\n",
        "    # |  cuda  | torch.float32  |           none          | scripted model |  0.8628  | 0.302331347 |\n",
        "    # |  cuda  | torch.float32  |        mirroring        | scripted model |  0.8728  | 0.323998472 |\n",
        "    # |  cuda  | torch.float32  |        translate        | scripted model |  0.8735  | 1.356375027 |\n",
        "    # |  cuda  | torch.float32  | mirroring_and_translate | scripted model |  0.8785  | 2.535533913 |\n",
        "    # +--------+----------------+-------------------------+----------------+----------+-------------+\n"
      ],
      "metadata": {
        "id": "shNhbqH1hvtd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The full setup."
      ],
      "metadata": {
        "id": "G4MXSbHDh7Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(model_path: str):\n",
        "    data = prepare_data(\"./data\")\n",
        "    model_types = (\n",
        "        \"raw model\",\n",
        "        \"scripted model\",\n",
        "        \"scripted model\",\n",
        "        \"traced model\",\n",
        "        \"traced model\",\n",
        "        \"frozen model\",\n",
        "        \"frozen model\",\n",
        "        \"optimized for inference\",\n",
        "        \"optimized for inference\",\n",
        "        \"compiled model\",\n",
        "        \"compiled model\",\n",
        "    )\n",
        "    dtypes = (\n",
        "        torch.bfloat16,\n",
        "        torch.half,\n",
        "        torch.float32\n",
        "    )\n",
        "    tta_types = (\n",
        "        \"none\",\n",
        "        \"mirroring\",\n",
        "        \"translate\",\n",
        "        \"mirroring_and_translate\",\n",
        "    )\n",
        "    devices = (\n",
        "        torch.accelerator.current_accelerator() if torch.accelerator.is_available() else None,\n",
        "        torch.device(\"cpu\"),\n",
        "    )\n",
        "\n",
        "    do_speed_test(data, model_types, dtypes, tta_types, devices, model_path)\n",
        "    do_tta_test(data, model_types, dtypes, tta_types, devices, model_path)\n"
      ],
      "metadata": {
        "id": "DIPsR49ih6BI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the inference file"
      ],
      "metadata": {
        "id": "U1ybrPrLiC51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "![ ! -f best.pth ] && curl -L -o best.pth https://raw.githubusercontent.com/Tensor-Reloaded/AI-Learning-Hub/main/resources/advanced_pytorch/checkpoints/best.pth"
      ],
      "metadata": {
        "id": "c8NOTGjhh_pF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "    model_path = \"best.pth\"\n",
        "    main(model_path)\n"
      ],
      "metadata": {
        "id": "w29Q-OE_h-ZW",
        "outputId": "e74558ad-0e6f-421c-b65d-b8a0af1a88bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.4MB/s]\n",
            "Speed experiments:  12%|█▏        | 8/66 [02:03<11:42, 12.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type optimized for inference failed on torch.bfloat16 on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSpeed experiments:  14%|█▎        | 9/66 [02:04<08:08,  8.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type optimized for inference failed on torch.bfloat16 on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0930 12:17:15.594000 381 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "Speed experiments:  17%|█▋        | 11/66 [02:55<15:14, 16.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------------+----------+-------------------------+----------+--------------+\n",
            "| Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed    |\n",
            "+--------+----------------+----------+-------------------------+----------+--------------+\n",
            "|  cuda  | torch.bfloat16 |   none   |        raw model        |  0.8628  | 16.884628199 |\n",
            "|  cuda  | torch.bfloat16 |   none   |      scripted model     |  0.8628  | 16.588373199 |\n",
            "|  cuda  | torch.bfloat16 |   none   |      scripted model     |  0.8628  | 16.38698488  |\n",
            "|  cuda  | torch.bfloat16 |   none   |       traced model      |  0.8628  | 16.580808231 |\n",
            "|  cuda  | torch.bfloat16 |   none   |       traced model      |  0.8628  | 16.387139424 |\n",
            "|  cuda  | torch.bfloat16 |   none   |       frozen model      |  0.8627  | 16.359960733 |\n",
            "|  cuda  | torch.bfloat16 |   none   |       frozen model      |  0.8627  | 16.39242669  |\n",
            "|  cuda  | torch.bfloat16 |   none   | optimized for inference |   N/A    |     N/A      |\n",
            "|  cuda  | torch.bfloat16 |   none   | optimized for inference |   N/A    |     N/A      |\n",
            "|  cuda  | torch.bfloat16 |   none   |      compiled model     |  0.8628  | 28.684508774 |\n",
            "|  cuda  | torch.bfloat16 |   none   |      compiled model     |  0.8628  | 16.540117473 |\n",
            "+--------+----------------+----------+-------------------------+----------+--------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Speed experiments:  29%|██▉       | 19/66 [03:03<01:26,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type optimized for inference failed on torch.float16 on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSpeed experiments:  30%|███       | 20/66 [03:03<01:10,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type optimized for inference failed on torch.float16 on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Speed experiments:  33%|███▎      | 22/66 [03:05<00:52,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+----------+-------------------------+----------+-------------+\n",
            "| Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
            "+--------+---------------+----------+-------------------------+----------+-------------+\n",
            "|  cuda  | torch.float16 |   none   |        raw model        |  0.8628  | 0.456860901 |\n",
            "|  cuda  | torch.float16 |   none   |      scripted model     |  0.8627  | 0.245520422 |\n",
            "|  cuda  | torch.float16 |   none   |      scripted model     |  0.8627  |  0.24478505 |\n",
            "|  cuda  | torch.float16 |   none   |       traced model      |  0.8628  | 0.297093739 |\n",
            "|  cuda  | torch.float16 |   none   |       traced model      |  0.8628  | 0.298865424 |\n",
            "|  cuda  | torch.float16 |   none   |       frozen model      |  0.8629  | 0.189003802 |\n",
            "|  cuda  | torch.float16 |   none   |       frozen model      |  0.8629  | 0.188668645 |\n",
            "|  cuda  | torch.float16 |   none   | optimized for inference |   N/A    |     N/A     |\n",
            "|  cuda  | torch.float16 |   none   | optimized for inference |   N/A    |     N/A     |\n",
            "|  cuda  | torch.float16 |   none   |      compiled model     |  0.8628  | 0.351826348 |\n",
            "|  cuda  | torch.float16 |   none   |      compiled model     |  0.8628  | 0.245900978 |\n",
            "+--------+---------------+----------+-------------------------+----------+-------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Speed experiments:  50%|█████     | 33/66 [03:20<00:42,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+----------+-------------------------+----------+-------------+\n",
            "| Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed   |\n",
            "+--------+---------------+----------+-------------------------+----------+-------------+\n",
            "|  cuda  | torch.float32 |   none   |        raw model        |  0.8628  | 0.612244879 |\n",
            "|  cuda  | torch.float32 |   none   |      scripted model     |  0.8628  | 0.714759405 |\n",
            "|  cuda  | torch.float32 |   none   |      scripted model     |  0.8628  | 0.589165576 |\n",
            "|  cuda  | torch.float32 |   none   |       traced model      |  0.8628  | 0.616923644 |\n",
            "|  cuda  | torch.float32 |   none   |       traced model      |  0.8628  | 0.614003439 |\n",
            "|  cuda  | torch.float32 |   none   |       frozen model      |  0.8628  | 0.560715173 |\n",
            "|  cuda  | torch.float32 |   none   |       frozen model      |  0.8628  | 0.559296638 |\n",
            "|  cuda  | torch.float32 |   none   | optimized for inference |  0.8628  |  0.55052995 |\n",
            "|  cuda  | torch.float32 |   none   | optimized for inference |  0.8628  | 0.551463229 |\n",
            "|  cuda  | torch.float32 |   none   |      compiled model     |  0.8628  | 0.591989673 |\n",
            "|  cuda  | torch.float32 |   none   |      compiled model     |  0.8628  | 0.589906088 |\n",
            "+--------+---------------+----------+-------------------------+----------+-------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Speed experiments:  67%|██████▋   | 44/66 [05:15<03:34,  9.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------------+----------+-------------------------+----------+--------------+\n",
            "| Device |     Dtype      | TTA Type |        Model Type       | Accuracy |   Elapsed    |\n",
            "+--------+----------------+----------+-------------------------+----------+--------------+\n",
            "|  cpu   | torch.bfloat16 |   none   |        raw model        |  0.8628  | 12.251049287 |\n",
            "|  cpu   | torch.bfloat16 |   none   |      scripted model     |  0.8628  | 10.502742393 |\n",
            "|  cpu   | torch.bfloat16 |   none   |      scripted model     |  0.8628  | 10.219771897 |\n",
            "|  cpu   | torch.bfloat16 |   none   |       traced model      |  0.8628  | 9.444874722  |\n",
            "|  cpu   | torch.bfloat16 |   none   |       traced model      |  0.8628  | 10.067524619 |\n",
            "|  cpu   | torch.bfloat16 |   none   |       frozen model      |  0.8628  | 9.681582857  |\n",
            "|  cpu   | torch.bfloat16 |   none   |       frozen model      |  0.8628  |  9.74893516  |\n",
            "|  cpu   | torch.bfloat16 |   none   | optimized for inference |  0.8628  | 8.125695756  |\n",
            "|  cpu   | torch.bfloat16 |   none   | optimized for inference |  0.8628  |  6.87960141  |\n",
            "|  cpu   | torch.bfloat16 |   none   |      compiled model     |  0.8628  | 9.944255357  |\n",
            "|  cpu   | torch.bfloat16 |   none   |      compiled model     |  0.8628  | 9.934951253  |\n",
            "+--------+----------------+----------+-------------------------+----------+--------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Speed experiments:  83%|████████▎ | 55/66 [07:06<01:49,  9.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+----------+-------------------------+----------+--------------+\n",
            "| Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed    |\n",
            "+--------+---------------+----------+-------------------------+----------+--------------+\n",
            "|  cpu   | torch.float16 |   none   |        raw model        |  0.8628  | 10.195400691 |\n",
            "|  cpu   | torch.float16 |   none   |      scripted model     |  0.8628  | 10.132698804 |\n",
            "|  cpu   | torch.float16 |   none   |      scripted model     |  0.8628  | 9.415232976  |\n",
            "|  cpu   | torch.float16 |   none   |       traced model      |  0.8628  | 9.189693126  |\n",
            "|  cpu   | torch.float16 |   none   |       traced model      |  0.8628  | 9.873023653  |\n",
            "|  cpu   | torch.float16 |   none   |       frozen model      |  0.8628  | 9.554109585  |\n",
            "|  cpu   | torch.float16 |   none   |       frozen model      |  0.8628  | 9.601258977  |\n",
            "|  cpu   | torch.float16 |   none   | optimized for inference |  0.8628  | 7.365994933  |\n",
            "|  cpu   | torch.float16 |   none   | optimized for inference |  0.8628  | 6.933587296  |\n",
            "|  cpu   | torch.float16 |   none   |      compiled model     |  0.8628  | 10.214564148 |\n",
            "|  cpu   | torch.float16 |   none   |      compiled model     |  0.8628  | 10.10502345  |\n",
            "+--------+---------------+----------+-------------------------+----------+--------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Speed experiments: 100%|██████████| 66/66 [08:59<00:00,  8.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+----------+-------------------------+----------+--------------+\n",
            "| Device |     Dtype     | TTA Type |        Model Type       | Accuracy |   Elapsed    |\n",
            "+--------+---------------+----------+-------------------------+----------+--------------+\n",
            "|  cpu   | torch.float32 |   none   |        raw model        |  0.8628  | 10.396136796 |\n",
            "|  cpu   | torch.float32 |   none   |      scripted model     |  0.8628  | 10.187017377 |\n",
            "|  cpu   | torch.float32 |   none   |      scripted model     |  0.8628  | 9.242592588  |\n",
            "|  cpu   | torch.float32 |   none   |       traced model      |  0.8628  |  9.72705402  |\n",
            "|  cpu   | torch.float32 |   none   |       traced model      |  0.8628  | 11.291153583 |\n",
            "|  cpu   | torch.float32 |   none   |       frozen model      |  0.8628  | 10.222028228 |\n",
            "|  cpu   | torch.float32 |   none   |       frozen model      |  0.8628  | 9.580895448  |\n",
            "|  cpu   | torch.float32 |   none   | optimized for inference |  0.8628  |  7.60580902  |\n",
            "|  cpu   | torch.float32 |   none   | optimized for inference |  0.8628  | 6.929277715  |\n",
            "|  cpu   | torch.float32 |   none   |      compiled model     |  0.8628  | 9.964773122  |\n",
            "|  cpu   | torch.float32 |   none   |      compiled model     |  0.8628  | 9.956867878  |\n",
            "+--------+---------------+----------+-------------------------+----------+--------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TTA experiments:   8%|▊         | 1/12 [00:17<03:08, 17.17s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgo9Z9RBdH5z"
      },
      "source": [
        "The full inference script is available in [inference_optimization_and_tta.py](./inference_optimization_and_tta.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises:\n",
        "1. Test this inference pipeline for a better performing model.\n",
        "2. Export a model trained with your pipeline to ONNX. Check the performance gains.\n",
        "3. Load a torch scripted model in C++ using a libtorch docker container. Check the performance gains."
      ],
      "metadata": {
        "id": "muvMh9Zuiqns"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiVkCtJgdH5z"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW74vX_KdH50"
      },
      "source": [
        "| All     | [advanced_pytorch/](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch) |\n",
        "|---------|-- |\n",
        "| Current | [Inference Optimization And TTA](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch/InferenceOptimizationAndTTA.ipynb) |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}